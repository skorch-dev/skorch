{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea00c9e-ab6a-4a56-8e91-782b06002f6c",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT model with skorch and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4d0cc-40c5-48a9-bd52-fdd522498acf",
   "metadata": {},
   "source": [
    "In this notebook, we follow the fine-tuning guideline from [Hugging Face documentation](https://huggingface.co/docs/transformers/training). Please check it out if we you want to know more about BERT and fine-tuning. Here, we assume that you're familiar with the general ideas.\n",
    "\n",
    "You will learn how to:\n",
    "- integrate the [Hugging Face transformers](https://huggingface.co/docs/transformers/index) library with skorch\n",
    "- use skorch to fine-tune a BERT model on a text classification task\n",
    "- use skorch with the [Hugging Face accelerate](https://huggingface.co/docs/accelerate/index) library for automatic mixed precision (AMP) training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bfcd7",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/Hugging_Face_Finetuning.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\" href=\"https://github.com/skorch-dev/skorch/blob/master/notebooks/Hugging_Face_Finetuning.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737e20e",
   "metadata": {},
   "source": [
    "The first part of the notebook requires hugginface `transformers` as an additional dependency. If you have not already installed it, you can do so like this:\n",
    "\n",
    "`python -m pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465f48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -z \"$COLAB_GPU\" ] && pip install torch \"skorch>=0.12\" transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0f846-ea36-4a07-835d-c2b8c69c27fd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5164c02c-9d4f-4b1c-bf72-0e8f21a23b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import LRScheduler, ProgressBar\n",
    "from skorch.hf import HuggingfacePretrainedTokenizer\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c4fee-8c1d-42e2-933f-27570d1c7ea3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c77bdf-9929-47b7-8e19-39b623fd4a52",
   "metadata": {},
   "source": [
    "Change the values below if you want to try out different model architectures and hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f17a28-52e3-4f0c-8c3e-308ecc9288f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a tokenizer and BERT model that work together\n",
    "TOKENIZER = \"distilbert-base-uncased\"\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"\n",
    "\n",
    "# model hyper-parameters\n",
    "OPTMIZER = torch.optim.AdamW\n",
    "LR = 5e-5\n",
    "MAX_EPOCHS = 3\n",
    "CRITERION = nn.CrossEntropyLoss\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ed4ac-27e8-4b80-bcc6-2431f1d9ec12",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ecfd881-f589-493a-a028-68becdf240ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e33d0d-5bfb-49ce-82e7-d0e05d10c0e6",
   "metadata": {},
   "source": [
    "For this notebook, we're making use the 20 newsgroups dataset. It is a text classification dataset with 20 classes. A decent score would be to reach 89% accuracy out of sample. For more details, read the description below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d84f34-b506-40c1-a9da-2d40c0a2feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.DESCR.split('Usage')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec3f23b-7906-4804-a517-db17001c22e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061eb1d4-4805-4a7f-91fe-b64b43939905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b491705e-95b0-4699-86b0-71128e7d54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ec04ba-57fd-47bf-9528-1dc56fcad7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\\nLines: 53\\nNntp-Posting-Host: alban.dsv.su.se\\nReply-To: hilmi-er@dsv.su.se (Hilmi Eren)\\nOrganization: Dept. of Computer and Systems Sciences, Stockholm University\\n\\n\\n  \\n|>      henrik@quayle.kpc.com writes:\\n\\n\\n|>\\tThe Armenians in Nagarno-Karabagh are simply DEFENDING their RIGHTS\\n|>        to keep their homeland and it is the AZERIS that are INVADING their \\n|>        territorium...\\n\\t\\n\\n\\tHomeland? First Nagarno-Karabagh was Armenians homeland today\\n\\tFizuli, Lacin and several villages (in Azerbadjan)\\n\\tare their homeland. Can\\'t you see the\\n\\tthe  \"Great Armenia\" dream in this? With facist methods like\\n\\tkilling, raping and bombing villages. The last move was the \\n\\tblast of a truck with 60 kurdish refugees, trying to\\n\\tescape the from Lacin, a city that was \"given\" to the Kurds\\n\\tby the Armenians. \\n\\n\\n|>       However, I hope that the Armenians WILL force a TURKISH airplane \\n|>       to LAND for purposes of SEARCHING for ARMS similar to the one\\n|>       that happened last SUMMER. Turkey searched an AMERICAN plane\\n|>       (carrying humanitarian aid) bound to ARMENIA.\\n|>\\n\\n\\tDon\\'t speak about things you don\\'t know: 8 American Cargo planes\\n\\twere heading to Armenia. When the Turkish authorities\\n\\tannounced that they were going to search these cargo \\n\\tplanes 3 of these planes returned to it\\'s base in Germany.\\n\\t5 of these planes were searched in Turkey. The content of\\n\\tof the other 3 planes? Not hard to guess, is it? It was sure not\\n\\thumanitarian aid.....\\n\\n\\tSearch Turkish planes? You don\\'t know what you are talking about.\\n\\tTurkey\\'s government has announced that it\\'s giving weapons\\n\\tto Azerbadjan since Armenia started to attack Azerbadjan\\n\\tit self, not the Karabag province. So why search a plane for weapons\\n\\tsince it\\'s content is announced to be weapons? \\n\\n\\n\\n\\n\\n\\nHilmi Eren\\nDept. of Computer and Systems Sciences, Stockholm University\\nSweden\\nHilmi-er@dsv.su.se\\n\\n\\n\\n\\n',\n",
       " 'Subject: VHS movie for sale\\nFrom: koutd@hirama.hiram.edu (DOUGLAS KOU)\\nOrganization: Hiram College\\nNntp-Posting-Host: hirama.hiram.edu\\nLines: 13\\n\\nVHS movie for sale.\\n\\nDance with Wovies\\t($12.00)\\n\\nThe tape is new and just open, buyer pay shipping cost.\\nIf you are interested, please send your offer to\\nkoutd@hirama.hiram.edu\\n\\nthanks,\\n\\nDouglas Kou\\nHiram College\\n\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a8f51-98bc-4f09-b14d-5708a1788b80",
   "metadata": {},
   "source": [
    "## Prepare the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c0f71-4c62-4c3c-b411-02075d81169f",
   "metadata": {},
   "source": [
    "We want to use a linear learning rate schedule that linearly decreases the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acde3950-a9dc-434f-b76c-587e98fa8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = MAX_EPOCHS * (len(X_train) // BATCH_SIZE + 1)\n",
    "\n",
    "def lr_schedule(current_step):\n",
    "    factor = float(num_training_steps - current_step) / float(max(1, num_training_steps))\n",
    "    assert factor > 0\n",
    "    return factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207018e-f281-4fff-b394-b35d41694bad",
   "metadata": {},
   "source": [
    "Next we wrap the BERT module itself inside a simple `nn.Module`. The only real work for us here is to load the pretrained model and to return the _logits_ from the model output. The rest of the outputs is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e0924d-65d4-4fd0-af48-338acf40ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModule(nn.Module):\n",
    "    def __init__(self, name, num_labels):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.reset_weights()\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.name, num_labels=self.num_labels\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        pred = self.bert(**kwargs)\n",
    "        return pred.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6234ba",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f08038",
   "metadata": {},
   "source": [
    "We make use of `HuggingfacePretrainedTokenizer`, which is a wrapper that skorch provides to use the tokenizers from Hugging Face. In this instance, we use a tokenizer that was pretrained in conjunction with BERT. The tokenizer is automatically downloaded if not already present. More on Hugging Face tokenizers can be found [here](https://huggingface.co/docs/tokenizers/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87512ddc-8e1e-4db1-b203-f26e6e8f3730",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82675bea-7f6e-470b-a468-e803a94cfcf3",
   "metadata": {},
   "source": [
    "### Putting it all togther"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a626328-b4a1-445c-9fad-5347263e889e",
   "metadata": {},
   "source": [
    "Now we can put together all the parts from above. There is nothing special going on here, we simply use an sklearn `Pipeline` to chain the `HuggingfacePretrainedTokenizer` and the neural net. Using skorch's `NeuralNetClassifier`, we make sure to pass the `BertModule` as the first argument and to set the number of labels based on `y_train`. The criterion is `CrossEntropyLoss` because we return the logits. Moreover, we make use of the learning rate schedule we defined above, and we add the `ProgressBar` callback to monitor our progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b798511a-c389-4a77-a77a-35ecc7c30e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tokenizer', HuggingfacePretrainedTokenizer(TOKENIZER)),\n",
    "    ('net', NeuralNetClassifier(\n",
    "        BertModule,\n",
    "        module__name=PRETRAINED_MODEL,\n",
    "        module__num_labels=len(set(y_train)),\n",
    "        optimizer=OPTMIZER,\n",
    "        lr=LR,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        criterion=CRITERION,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        iterator_train__shuffle=True,\n",
    "        device=DEVICE,\n",
    "        callbacks=[\n",
    "            LRScheduler(LambdaLR, lr_lambda=lr_schedule, step_every='batch'),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19feb6-aeaf-43f7-8409-1baebe9a4c77",
   "metadata": {},
   "source": [
    "Since we are using skorch, we could now take this pipeline to run a grid search or other kind of hyper-parameter sweep to figure out the best hyper-parameters for this model. E.g. we could try out a different BERT model or a different `max_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c78c9-6941-4de9-a43c-d4f154bdd13e",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3159ab90-1c5e-411f-937a-5ac9addbedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e8d0e55-dd20-4f13-ac3f-4ff9b807cf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss       dur\n",
      "-------  ------------  -----------  ------------  --------\n",
      "      1        \u001b[36m1.0325\u001b[0m       \u001b[32m0.8444\u001b[0m        \u001b[35m0.5309\u001b[0m  129.3804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3306\u001b[0m       \u001b[32m0.8780\u001b[0m        \u001b[35m0.4194\u001b[0m  129.7384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.1346\u001b[0m       \u001b[32m0.8798\u001b[0m        \u001b[35m0.4100\u001b[0m  129.8741\n",
      "CPU times: user 6min 7s, sys: 42.8 s, total: 6min 50s\n",
      "Wall time: 6min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tokenizer',\n",
       "                 HuggingfacePretrainedTokenizer(tokenizer='distilbert-base-uncased')),\n",
       "                ('net',\n",
       "                 <class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=BertModule(\n",
       "    (bert): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddin...\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385305a-af65-4c19-b5a3-809f933e7794",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc50f549-ce59-4440-a1e4-c5b30c4e8e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 s, sys: 23.6 ms, total: 19.5 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.inference_mode():\n",
    "    y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb04fc96-baa3-4592-a236-a022c96d1e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9006716154118063"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799b81f-57d5-4239-ba7b-82cd2f76ed0d",
   "metadata": {},
   "source": [
    "We can be happy with the results. We set ourselves the goal to reach or exceed 89% accuracy on the test set and we managed to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bd2da-89ef-4684-a45c-7ccc44fb7596",
   "metadata": {},
   "source": [
    "## Training with automatic mixed precision (AMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8376427-6d9d-421b-9078-3357f041726d",
   "metadata": {},
   "source": [
    "For this to work, you need:\n",
    "- A GPU that is capable of mixed precision training\n",
    "- The [accelerate library](https://huggingface.co/docs/accelerate/index), which you can install as: `python -m pip install 'accelerate>=0.11'`.\n",
    "- skorch version 0.12 or installed from the current master branch (`python -m pip install git+https://github.com/skorch-dev/skorch.git`)\n",
    "\n",
    "Again, we assume that you're familiar with the general concept of mixed precision training. For more information on how skorch integrates with accelerate, please consult the [skorch docs](https://skorch.readthedocs.io/en/latest/user/huggingface.html#accelerate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6981f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! [ ! -z \"$COLAB_GPU\" ] && pip install 'accelerate>=0.11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47aa1a6-f466-4a2c-84ab-034e4d6bdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from skorch.hf import AccelerateMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63dfa5a1-c75b-4b61-93b6-cedc203ce9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcceleratedNet(AccelerateMixin, NeuralNetClassifier):\n",
    "    \"\"\"NeuralNetClassifier with accelerate support\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eeac984-cc13-48e7-979f-9e9f208211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision='fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783df165-369c-448b-8867-12a8eb85daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([\n",
    "    ('tokenizer', HuggingfacePretrainedTokenizer(TOKENIZER)),\n",
    "    ('net', AcceleratedNet(                   # <= changed\n",
    "        BertModule,\n",
    "        accelerator=accelerator,              # <= changed\n",
    "        module__name=PRETRAINED_MODEL,\n",
    "        module__num_labels=len(set(y_train)),\n",
    "        optimizer=OPTMIZER,\n",
    "        lr=LR,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        criterion=CRITERION,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        iterator_train__shuffle=True,\n",
    "        # device=DEVICE,                      # <= changed\n",
    "        callbacks=[\n",
    "            LRScheduler(LambdaLR, lr_lambda=lr_schedule, step_every='batch'),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "123f7967-e2d5-4bff-877d-cef395af9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f7e22db-eced-4360-ab2d-92ea9d1dbe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.0463\u001b[0m       \u001b[32m0.8374\u001b[0m        \u001b[35m0.5547\u001b[0m  71.2980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3264\u001b[0m       \u001b[32m0.8786\u001b[0m        \u001b[35m0.4251\u001b[0m  73.2230\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.1387\u001b[0m       \u001b[32m0.8845\u001b[0m        \u001b[35m0.4142\u001b[0m  74.4516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tokenizer',\n",
       "                 HuggingfacePretrainedTokenizer(tokenizer='distilbert-base-uncased')),\n",
       "                ('net',\n",
       "                 <class '__main__.AcceleratedNet'>[initialized](\n",
       "  module_=BertModule(\n",
       "    (bert): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(...\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  ),\n",
       "))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fda6530-49b4-4423-b6fa-cf941dbf91db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 4.97 ms, total: 11.7 s\n",
      "Wall time: 7.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.inference_mode():\n",
    "    y_pred = pipeline2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab4ce9f3-85ab-4b6a-af6c-b7926320d4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.897136797454931"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4163616-3b62-46b5-976d-355378209fa7",
   "metadata": {},
   "source": [
    "Using AMP, we could reduce our training and prediction time by half, while attaining the same scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
